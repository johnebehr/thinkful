{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naïve Bayes\n",
    "Bayes Theorem covers the probabilistic relationship between multiple variables, and specifically allows you to define one conditional in terms of the underlying probabilities and the inverse condition.  It can be defined as: $$ P(y|x) = P(y)P(x|y)/P(x) $$ In plain English this reads as “the probability of $ y $ given $ x $ equals the probability of $ y $ times the probability of $ x $ given $ y $ divided by the probability of $ x $.  ”The theorem can be extended to when $ x $ is a vector (containing the multiple $ x $ variables use as inputs for the model to: $$ P(y|x_1,...,x_n) = P(y)P(x_1,...,x_n|y)/P(x_1,...,x_n) $$ This explains the relationship of an outcome to a vector of conditions rather than to a single other event.  This can be read as the probability of $ y $, in the case of the model, the categorical outcome you are interested in, given a set of observations is equal to the probability of that set of observations given $ y $ divided by the probability of that set of outcomes.\n",
    "Naïve refers to the assumption that any pair of variables in the conditional vector (the $ x $ variables) are independent from each other.\n",
    "That independence does something important to the vectorized Bayes Rule equation.  It allows you to break up that large $ P(x_1,...,x_n|y) $ into the product of each individual condition.  Written out it would be something like: $$ P(y|x_1,...,x_n) = P(y)*P(x_1|y)*...*P(x_n|y)/P(x_1,...,x_n) $$ You can even simplify further because for any observation you are attempting to predict, the $ x $-vector will be constant, so that part of the probability simplifies out leaving: $$ P(y|x_1,...,x_n) \\approx P(y)*P(x_1|y)*...*P(x_n|y) $$ So $$ \\hat{y} = argmax_y(P(y)\\prod_{i=1}^nP(x_i|y)) $$ This states that the estimator of y is the maximum over $ y $ of the $ P(y)*\\prod_{i=1}^nP(x_i|y) $.  Naïve Bayes returns the $ y $ values that maximizes the following argument.  This means it returns a single value, or category.\n",
    "The estimate is the $ y $ that maximizes the argument because Naïve Bayes is used as a classifier.  You are interested in which $ y $ values is most likely to have give the observed set of $ x $ variables based on their Bayesian probabilities.  There are ways to modify the rule into returning probabilities, but these are generally not very accurate and not to be used.  It is said that this is a good classifier but not a good estimator.\n",
    "## Simple is Sometimes Better\n",
    "Assumptions have been made.  The assumption of independence between each pair is hugely important and rarely totally accurate.  Typically, the columns of a dataframe are not independent of each other.  However, Naïve Bayes is still used in the real world.\n",
    "There are a few reasons for that.  First, Naïve Bayes is simple.  It is easy to understand both how it operates and what it’s doing.  More importantly it is incredibly fast which can be very useful from a practical perspective.  It also relies on probabilities, which are based on counts, so you can actually train the classifier with more data than could fit into memory at one time.\n",
    "## Types of Naïve Bayes\n",
    "When running a Naïve Bayes classifier, you have to make another assumption around the distribution of $ P(x|y)  $.\n",
    "There are three main classifiers: Bernoulli, Multinomial, and Gaussian Naïve Bayes.  Each classifier assumes that the distribution of the conditional is the given distribution.  These distributions have limitations.  A binomial only takes two possible values.  A multinomial has discrete outcomes, and a Gaussian (aka “normal”) takes values along the continuous normal distribution.\n",
    "Choosing which kind of classifier, you want to use depends on the distribution of the outcome variable.  Choose the distribution that best fits the data.\n",
    "## Improving Performance\n",
    "In running this type of classifier, there are many things you can do to improve the performance of the model.  First and foremost, feature engineering.  This is particularly true in text-based problems.  It is largely up to the creativity and knowledge of the one building the model to draw out the right features.\n",
    "In Naïve Bayes, feature select can also be important because features are equally weighted.  Heavily correlated features can lead to doubling the impact of what is essentially a single feature.  Remember the assumption that every pair of variables is independent of each other.  The more removed from that assumption reality is, the more problems you may run into.\n",
    "## Downsides of Naïve Bayes\n",
    "First and foremost is the assumption of independence.  It is a double-edged sword because not only is it a condition you often fail to have, but it also means that any time two variables affect the outcome most in concert the model will fail to see it.  This effect is called _interation_ and occurs when any two features create a different effect when they both have a specific value than they would as independent occurrences.  In Naïve Bayes any such interaction is lost.\n",
    "Naïve Bayes can only predict the outcome of categories it has seen before.  This applies to both the outcome and the inputs.  If you have new $ x $-values in the test, the model will default to ignoring that specific outcome.   Like all classifiers it cannot predict a class it hasn’t seen.  The way Naïve Bayes handles partial data, however, does have the benefit of being indifferent to missing datapoints.  Those missing datapoints simply get ignored, drawing what information it can from the other variables of that observation.\n",
    "## References\n",
    "Scikit-Lear Naïve Bayes: https://scikit-learn.org/stable/modules/naive_bayes.html <br />\n",
    "A Gentle Introduction to Naïve Bayes Classifier: https://medium.com/datadriveninvestor/a-gentle-introduction-to-naive-bayes-classifier-9d7c4256c999 <br />\n",
    "Introduction to Bayesian Thinking: from Bayes theorem to Bayes networks: https://towardsdatascience.com/will-you-become-a-zombie-if-a-99-accuracy-test-result-positive-3da371f5134 <br />\n",
    "The Bayesian Trap: https://youtu.be/R13BD8qKeTg <br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}