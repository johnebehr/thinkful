{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21.04 Random Forest Models\n",
    "\n",
    "_Decision Trees_ are great, and their easily represented set of rules is a powerful feature for modeling and even more so for conveying that model to a more general audience.  But their high variance and propensity to overfit are serious problems.  Luckily there is a way you can handle both of those issues and create an even more powerful kind of model.\n",
    "\n",
    "### What is a Random Forest?\n",
    "What if instead of making one decision tree you made several.  As many as you wanted, really.  A whole forest.  Then each tree in the forest got a vote on the outcome for a given observation.  Then you'd have a new model type: **Random Forest**.  Random forests have become an incredibly popular technique for data scientists because it to be a top performer in a huge number of circumstances, with low variance and high accuracy.\n",
    "\n",
    "Much like Decision Trees, Random Forest can be used for both classification and regression problems.  The main difference is how the votes are aggregated.  As a classifier the most popular outcome (the mode) is returned.  As a regression it is typically the average or mean that is returned.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "When building a Random Forest, you get to set parameters for both the tree and the forest.  So for the tree you have the same parameters as before: you can set the depth of the tree and the number of features used in each rule or split.  You can also specify how the tree is built, with using information gain and entropy, like you did in notebook 20.03, or other methods like [Gini impurity](https://www.garysieling.com/blog/sklearn-gini-vs-entropy-criteria).\n",
    "\n",
    "You also get to control the number of estimators you want to generate, or the number of trees in the forest.  Here you have a tradeoff between how much variance you can explain and the computational complexity.  This is pretty easily tunable.  As you increase the number of trees in the forest the accuracy should converge as eventually the additional learning from another tree approaches zero.  There isn't an infinite amount of information to learn, and at some point the trees have learned all they can.  So when you have acceptable variance in accuracy you can stop adding trees.  This becomes worthwhile when you're dealing with large datasets with many variables.\n",
    "\n",
    "### Bagging and Random Subspace\n",
    "\n",
    "Now, Random Forest models donâ€™t just create a ton of trees using the same data again and again and again.  Instead they use bagging and random subspace to generate trees that are different.  Without this, the trees could be incredibly similar (even identical), leading to correlation between trees and vulnerability to bias in the trees from some highly predictive features dominating every tree, creating a series of very similar trees with very similar, and potentially biased predictions.\n",
    "\n",
    "First, Random Forest use **bagging**.  Each tree selects a subset of observations with replacement to build the training set.  Replacement here means it can simply choose the same observation multiple time, which is only really a problem when there are few observations.  It puts the observation \"back in the bag\", where it can be pulled and chose again.\n",
    "\n",
    "Random Forests also typically use a random subset of features for each split.  This means for each time it must perform a split or generate a rule, it is only looking at the **random subspace** created by a random subset of some of the features as possibilities ot generate that rule.  This will help avoid the aforementioned correlation problem because the trees will not be built with the same available features at every point.  As a rule, for a dataset with $x$ features $\\sqrt{x}$ features are used for classifiers and $x/3$ for regression.\n",
    "\n",
    "### Advantages and Disadvantages of Random Forests\n",
    "\n",
    "The biggest advantage of Random Forest is its tendency to be a very strong performer.  It is reasonably accurate in a myriad of situations from regression to classification.  It's very popular but not without its disadvantages.  \n",
    "\n",
    "First, in either classification or regression it will not predict outside of sample, meaning it will only return values that are within a range it has seen before.  Random Forests can also get rather large and slow if you let them grow too wildly.\n",
    "\n",
    "The biggest disadvantage, however, is the lack of transparency in the process.  Random Forest is often referred to as a \"**black box**\" model, meaning it will give you an output but very little insight into how it got there.  You'll run into a few mor of these throughout the course.\n",
    "\n",
    "Black Box models often make the more statistically minded data scientists nervous.  You don't get much insight into the process.  You can't see the rules it's really applying or what variables it's prioritizing or how.  You don't see any of the internal process or get to look \"inside the box\".  Therefore you also can't represent that process in a simple visual form or learn about the underlying process.  You have to trust in the algorithm building the trees and the lack of variance from a large number of them being generated.  It usually works out pretty well, and you can of course evaluate the model via other methods to validate your conclusions.  The next notebook (21.05) will walk through a Random Forest Classifier. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}