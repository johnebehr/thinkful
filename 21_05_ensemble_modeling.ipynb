{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21.05 Ensemble Modeling\n",
    "\n",
    "Introducing Random Forests leads to another, much larger, topic: **Ensemble models**.  Ensemble models are essentially models made up of other models.  These component models are often models that are simpler than would be necessary to accurately predict the desired outcome on their own.  In the case of Random Forest, those sub models are Decision Trees.  Random Forest generates many Decision Trees and combines them to generate a single prediction via a voting process.  \n",
    "\n",
    "### Methods of Ensemble Modeling \n",
    "\n",
    "There are many kinds of ensemble models.  In fact, there are infinite kinds of ensemble models because you can combine most kinds of models together and create a new kind of ensemble model by mixing and re-mixing different component models.  However, most ensemble models fall into three main categories.  \n",
    "\n",
    "**Bagging** is one ensemble technique.  In bagging you take subsets of the data and train a model on each subset.  Then the subsets are allowed to simultaneously vote on the outcome, either taking a majority or a mean.  This is how Random Forests work.\n",
    "\n",
    "Another ensemble technique is called **boosting**.  Rather than build multiple models simultaneously like bagging, boosting uses the output of one model as an input into the next in a form of serial processing.  These models then get daisy-chained together sequentially until some stopping condition is met.  Boosting will be covered later.\n",
    "\n",
    "Finally, **stacking** is a two phase process.  In the first phase multiple models are trained in parallel.  Then in the second phase those models are used as inputs into a final model to give your prediction.  This approach combines the parallel approach embodied by bagging with the serial approach of boosting to create a hybrid of the two.  \n",
    "\n",
    "You can create your own ensemble method by manually combining models, but there are already several widely used forms of ensemble learning in use.  These will be covered later\n",
    "\n",
    "### _Think Like a Data Scientist_\n",
    "\n",
    "_There are advantages and disadvantages to ensemble models.  Most notable is their performance.  Ensemble models are often some of the most accurate techniques to apply to a problem.  They also tend to have low variance because they're built from multiple internal models._\n",
    "\n",
    "_However, there are also downsides.  Most notable, some ensemble techniques, particularly boosting, are prone to overfitting.  You also lose a lot of the transparency that individual models offer.  You will see this clearly in the Random Forest example, where the easy explanations offered by decision trees are abstracted away by the forest._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}