{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is a regression task?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Supervised learning deals with two broad types of problems __regression__ and __classification__.  In a classification, the target variable can take a limited number of values, which are called categories or classes.  Regression problems require a different approach, instead of classifying you are quantifying.  With regression problems you are interested in an outcome that, in theory, can take on an infinite number of values. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Regression vs. Classification \n",
    "In a classification problem, the target variable is categorical, the variable only takes discrete values from a specified set.  Regression problems have a continuous outcome variable that can sometimes be bounded but can have an infinite range of values between those bounds or no bounds at all.  Examples of _regression tasks_ include stock price prediction, revenue prediction, weather forecasting, and demand prediction.  With regression problems you are interested in outcome variables that are continuous or have so man values that it's easiest to treat them as continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Formulating a Regression problem\n",
    "Linear regression models the relationship between one or more observed feature and some continuous target variable.  Based on observed data, you can try to derive a function that describes the target as a function of the feature set.  The function produces a line where you can look up feature values and find predicted target values.  \n",
    "\n",
    "What you want is a way to construct a mathematical description of the underlying relationship between feature and target.  Conceptually, you want to find a line that can be drawn through a scatter plot of known data that represents the relationship between the feature and the target reasonably well, so that it does a good job at predicting values that are directly observed in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How Does Linear Regression Work?\n",
    "In simple terms, if you start with a target and feature and plot each observation on an x- and y-axis, and then draw a line that best fits the observed cases, the fitted line represents the linear relationship between the two variables.  The line represents a linear function."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building Regression Models as an Iterative Process\n",
    "\n",
    "1. __Designing a Model:__\n",
    "\n",
    "  The first step is to choose an initial set of features that you believe have a relationship with the target variable.  Choosing the initial feature set stems from the _feature engineering_ and _exploratory data analysis_ that you typically do before the model selection.  Assume though, you only have two features ($x$ and $z$) and one target ($y$), the initial model can be expressed in the following form: $ y = \\alpha + \\beta x + \\theta z $\n",
    "  In the formula, $\\alpha$ is the __bias term__ and $\\beta$ and $\\theta$ are the __coefficients__.  \n",
    "\n",
    "2. __Estimating the Coefficients:__\n",
    "\n",
    "  The nest step is to find the optimal parameters of the model.  This is the same a drawing a fitted line.  Linear regression chooses the best line from an infinite number of lines using an optimization algorithm called __Ordinary Least Squares (OLS)__.\n",
    "\n",
    "  OLS is a relatively simple algorithm.  It tries to minimize the sum of the squared distances between each point and the line, and it chooses the line that minimizes this sum.  The minimization is the optimization that the linear regression method does.  After the optimization process, you get a function that is represented by a regression line in a chart.  In the literature this step is called estimation.  \n",
    "\n",
    "3. __Assumptions of Linear Regression:__\n",
    "\n",
    "  After you estimate the model's parameters, you need to make sure that some assumptions about the data and the model hold.  Particularly, there are a number of assumptions that must be met by the model in order to use linear regression, there are six.\n",
    "\n",
    "    1. The relationship between feature(s) and target(s) is linear.\n",
    "    2. The errors of the model should be equal to zero on average.\n",
    "    3. The model's errors are consistently distributed, which is know as __homoscedasticity__.\n",
    "    4. Features are at most only weakly correlated, there is not strong __multicollinearity__.\n",
    "    5. The model's errors should be uncorrelated with each other. \n",
    "    6. The features and model errors are independent of one another.\n",
    "\n",
    "4. ___Understanding the Relationship Between Features and Target:__\n",
    "\n",
    "  After the previous steps, you get some values for the coefficients of the features that capture the relationship between the features and the target variable.  You can quantify the relationship between the target variable and each feature using the estimated coefficients: $ y = \\alpha + \\beta x + \\theta z $\n",
    "\n",
    "5. __Evaluating Goodness of Fit:__\n",
    "  To measure how well a model explains the information in the outcome variable, look at the performance of the model in training data.  The  training data is comprised of the records that you use to estimate the model.  Unlike categorization problem where you use a confusion matrix to generate metrics, regression problems use metrics like R-squared or adjusted R-squared to evaluate the goodness of fit to the model.  If you are happy with the performance of the model, then you can proceed.  But if you think that the model doesn't explain the target variable very well, you need to change something in the model like adding some extra feature and go back to step 2 (estimation phase).\n",
    "\n",
    "6. __Predicting the Unknown:__\n",
    "\n",
    "  Regression models are quite flexible in predicting unknown target values.  This is a consequence of how you estimate the relationship between features and target variables.  Based on a finite set of observed cases, you generate a line that can, in principle, make a prediction about _any unobserved value_.\n",
    "\n",
    "7. __Measuring Test Performance:__\n",
    "\n",
    "  Now you have a function that can make predictions.  But will the prediction be reliable in practice?  To determine this, you need to evaluate the model on a test set.  The test set is data you hold back that the model was not trained on that you can use to see how well the model performs on new cases.\n",
    "\n",
    "  As in the case of measuring training performance, you can not use a confusion matrix to assess test set performance of regression models.  Instead, you use metrics like _Root Mean Squared Error (RMSE)_ to evaluate the models\n",
    "\n",
    "  Once you get the test set performance of the model, you can judge whether it is satisfactory or not.  If you think the model is doing a great job, then you can rely on the predictions it generates.  If you think that the model is not good enough, then you should go back to step 2 (estimation phase) and try other model alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From Simple to More Complex Linear Regression Models\n",
    "\n",
    "The discussion, up to now, has focused on a target and a feature.  In the real world you usually work with datasets that include more than one feature.  Linear regression models can handle many feature and even multiple targets.  These linear models are called __multivariable__ and __multivariate__ respectively.  In this discussion it is worth noting that the term _\"linear\"_ in the name of the linear regression model is there not to indicate that the fitted line is a linear one but instead to indicate that the linear relationship is between the target variable and the coefficients of the features.  In fact, linear regression models can estimate relationships that are _quadratic_, _cubic_, or any _polynomial_ order."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assignment\n",
    "\n",
    "To close out this checkpoint, answer the following questions.  Once you're done, you can compare your answers to the ones found in [this notebook, which you can download](https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/machine-learning-regression/solutions/1.solution_what_is_regression.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_1. Let's assume that you have World Bank data on financial, economic and social indicators for several countries. You want to measure the factors that affect the level of development in these countries. To this end, you decide to use per capita income as a proxy for the development level, which is defined as the national income divided by the population. You want to use some features in your dataset to predict per capita income. Is this task a classification or a regression task? Why? _\n",
    "\n",
    "  This is a regression task.  GDP is continuous by nature, though it could be put into discrete groupings.  For each country considered the GDP calculation returns a non-discrete value."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2. _Which of the following arguments are false and why?_\n",
    "    * _OLS is a special type of linear regression model._ \n",
    "    * _Regression models become useless if they don’t meet the assumptions of linear regression._ \n",
    "    * _Estimation and prediction are the same thing in the context of linear regression models._ \n",
    "    * _Linear regression is only one kind of regression model. Regression problems can also be solved with other kind of models like Support Vector Machines or Random Forests._\n",
    "\n",
    "  The third argument is false, Estimation and Prediction are not the same thing.  Within the context of linear regression, estimation is the process if fitting a line to observed date in order to allow a model to make predictions on unobserved data based on the linear relationship(s) between target(s) and variable(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3. _Assume that your project manager wants you to discover which free services your company offers make your customers buy more of your paid services. Formulate this task as a regression problem and write down the potential outcome and features that you’d like to work on._\n",
    "\n",
    "  This can be framed as a demand prediction. For the model $y$ is demand and each free service is represented by $ \\beta_n free\\_service_n $: $$ y = \\alpha + \\beta_1 free\\_service_1 + \\beta_2 free\\_service_2 + \\dots + \\beta_n free\\_service_n $$     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}